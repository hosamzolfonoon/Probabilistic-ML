{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b22e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method IndexOpsMixin.tolist of Index(['labels', 'predictions'], dtype='object')>\n",
      "Classes:  [0 1] => Number of classes =  2\n",
      "The number of examples per class: \n",
      "    Class 1 = 400 \n",
      "    Class 0 = 1000 \n",
      "\n",
      "True Predictions of Class 0 = 877\n",
      "False Predictions of Class 0 = 123\n",
      "True Predictions of Class 1 = 353\n",
      "False Predictions of Class 1 = 47\n",
      "TP rate (Recall / Sensitivity):  0.7415966386554622\n",
      "TN rate (Specificity):  0.9491341991341992\n",
      "FP rate (Fall-out):  0.050865800865800864\n",
      "FN rate (Miss rate):  0.25840336134453784\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "# Import txt as Pandas  Data Frame\n",
    "#df_labels = pd.read_csv('/home/hosam/Master/Probabilistic-ML/Exe01/Labels.txt', header=None)\n",
    "#df_pred   = pd.read_csv('/home/hosam/Master/Probabilistic-ML/Exe01/Y.txt', header=None)\n",
    "\n",
    "# Import txt as Pandas  Data Frame\n",
    "df_labels = pd.read_csv('/home/hosam/Master/Probabilistic-ML/Exe01/Labels2.txt', header=None)\n",
    "df_pred   = pd.read_csv('/home/hosam/Master/Probabilistic-ML/Exe01/Y2.txt', header=None)\n",
    "\n",
    "# Concatenating Data Frames\n",
    "df = pd.concat([df_labels, df_pred], axis=1)\n",
    "df.columns = [\"labels\", \"predictions\"]\n",
    "\n",
    "# Extracting classes\n",
    "classes = df['labels'].unique()\n",
    "print(df.columns.to_list)\n",
    "print('Classes: ', classes , '=> Number of classes = ', len(classes))\n",
    "\n",
    "# Answer Q A-1\n",
    "dict_q_a_1 = {}\n",
    "for cls in classes:\n",
    "    dict_q_a_1[cls] = (df['labels'] == cls).sum()\n",
    "print(f'The number of examples per class: \\n'\n",
    "      f'    Class 1 = {dict_q_a_1[1]} \\n'\n",
    "      f'    Class 0 = {dict_q_a_1[0]} \\n')\n",
    "\n",
    "# Answer Q A-2:5\n",
    "dict_q_a_2_5 = {}\n",
    "for y_obs in classes:\n",
    "    true_preds = ((df['labels'] == y_obs) & (df['predictions'] == y_obs)).sum()\n",
    "    false_preds = ((df['labels'] == y_obs) & (df['predictions'] != y_obs)).sum()\n",
    "    dict_q_a_2_5[f'True Predictions of Class {y_obs}'] = true_preds\n",
    "    dict_q_a_2_5[f'False Predictions of Class {y_obs}'] = false_preds\n",
    "    \n",
    "keys = list(dict_q_a_2_5.keys())\n",
    "for i in range(len(keys)):\n",
    "    print(f'{keys[i]} = {dict_q_a_2_5[keys[i]]}')\n",
    "\n",
    "TP_rate = (\n",
    "    dict_q_a_2_5['True Predictions of Class 1'] /\n",
    "    (dict_q_a_2_5['True Predictions of Class 1'] +\n",
    "     dict_q_a_2_5['False Predictions of Class 0'])\n",
    "    )\n",
    "print('TP rate (Recall / Sensitivity): ', TP_rate)\n",
    "\n",
    "TN_rate = (\n",
    "    dict_q_a_2_5['True Predictions of Class 0'] /\n",
    "    (dict_q_a_2_5['True Predictions of Class 0'] +\n",
    "     dict_q_a_2_5['False Predictions of Class 1'])\n",
    "    )\n",
    "print('TN rate (Specificity): ', TN_rate)\n",
    "\n",
    "FP_rate = (\n",
    "    dict_q_a_2_5['False Predictions of Class 1'] /\n",
    "    (dict_q_a_2_5['False Predictions of Class 1'] +\n",
    "     dict_q_a_2_5['True Predictions of Class 0'])\n",
    "    )\n",
    "print('FP rate (Fall-out): ', FP_rate)\n",
    "\n",
    "FN_rate = (\n",
    "    dict_q_a_2_5['False Predictions of Class 0'] /\n",
    "    (dict_q_a_2_5['False Predictions of Class 0'] +\n",
    "     dict_q_a_2_5['True Predictions of Class 1'])\n",
    "    )\n",
    "print('FN rate (Miss rate): ', FN_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40325c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[877 123]\n",
      " [ 47 353]]\n",
      "Accuracy: 0.8785714285714286\n",
      "Recall: 0.8825\n",
      "Precision: 0.7415966386554622\n",
      "F1: 0.8059360730593608\n"
     ]
    }
   ],
   "source": [
    "y_true = df['labels']\n",
    "y_pred = df['predictions']\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "# Recall (per class or average)\n",
    "recall = recall_score(y_true, y_pred)   # per class\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# F1-score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
